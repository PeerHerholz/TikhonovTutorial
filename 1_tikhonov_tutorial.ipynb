{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Encoding Models with Word Embeddings\n",
    "originally for NeuroHackademy 2020, by Alex Huth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some basic stuff we'll need later\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack the data we'll use\n",
    "!wget https://utexas.box.com/shared/static/8viy0g5ca9czbhlgyqtihtr2o37zjppd.zip\n",
    "!unzip 8viy0g5ca9czbhlgyqtihtr2o37zjppd.zip    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regression: what is it? Why would you use it?\n",
    "\n",
    "I'm interested in \"predictive\" models of brain responses. Suppose you do an experiment where you're recording some kind of brain responses, let's call them $y(t)$, while you provide your experimental subject with some kind of stimulus, let's call that $s(t)$. I think you can claim to understand something about what the brain is doing if you can build a model that predicts $y(t)$ from $s(t)$. (I think this claim is especially true if $s(t)$ is a \"natural\" stimulus, i.e. something that you would expect to see in everyday life, rather than something concocted specifically in a laboratory.)\n",
    "\n",
    "How do we build a model that predicts $y(t)$ from $s(t)$? There are lots of approaches! But they all use one core concept: **_regression_**.\n",
    "\n",
    "You do regression like this:\n",
    "### 1. Extract features ###\n",
    "You say \"I can break $s(t)$ into different parts, or features\". This transforms $s(t)$ into some other representation that we'll call $x(t)$. Let's say that the number of features in $x(t)$ is $p$.\n",
    "\n",
    "### 2. Define the model ###\n",
    "You say \"I can predict $y(t)$ from a weighted combination of the features in $x(t)$\". This means you are imagining a model that looks something like this,\n",
    "\n",
    "$$y(t) = \\sum_{i=1}^p x_i(t) \\beta_i + \\epsilon(t),$$\n",
    "where:\n",
    "* $x_i(t)$ refers to the $i$th feature in $x(t)$, \n",
    "* $\\beta_i$ refers to the weight on that feature (we don't know what this weight is yet), and \n",
    "* $\\epsilon(t)$ refers to the noise, i.e. any part of $y(t)$ that you can't predict from $x(t)$ (we don't know what this is yet either).\n",
    "\n",
    "Since this model is just a weighted sum, we can write it more simply using a little linear algebra,\n",
    "$$y(t) = x(t) \\beta + \\epsilon(t),$$\n",
    "where $x(t)$ is now a $1 \\times p$ vector of feature values and $\\beta$ is a $p \\times 1$ vector of weights.\n",
    "\n",
    "To deal with these equations more easily, we'll need to stack the values of $y(t)$ and $x(t)$ into matrices. Let's define those matrices like this:\n",
    "\n",
    "$$Y = \\begin{bmatrix} y(t=1) \\\\ y(t=2) \\\\ \\vdots \\\\ y(t=T) \\end{bmatrix}, \\;\n",
    "X = \\begin{bmatrix} x(t=1) \\\\ x(t=2) \\\\ \\vdots \\\\ x(t=T) \\end{bmatrix} $$\n",
    "\n",
    "So we now have $Y$, which is a $T \\times 1$ matrix of brain responses, and $X$, which is a $T \\times p$ matrix of features that we extracted from the stimuli. If we also define $\\epsilon$ as a vector of the same size as $Y$, then we can re-write the model like this:\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$\n",
    "\n",
    "Let's make some fake data using this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a simple regression! First we'll create some fake stimulus & \"true\" weights\n",
    "\n",
    "T_train = 100 # number of timepoints in our model training dataset\n",
    "p = 5 # number of features\n",
    "noise_size = 5.0 # the standard deviation of the noise, epsilon\n",
    "\n",
    "X_train = np.random.randn(T_train, p) # random Gaussian-distributed (normal) numbers\n",
    "\n",
    "beta_true = np.random.randn(p)\n",
    "\n",
    "Y_train = X_train.dot(beta_true) + noise_size * np.random.randn(T_train) # Y = X beta + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape\n",
    "# should be the number of timepoints, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimate the model weights, $\\beta$ ###\n",
    "You say \"I need to find the weights $\\beta$ that make my model good at predicting $y(t)$\". Here's where the magic is! To do this we're first going to define a **_loss function_** $\\mathcal{L}(\\beta)$, which tells us how good our model is for any particular value of the weight vector $\\beta$. (It's called a loss function because bigger values == bigger loss == a worse model. So we want the loss function to be as small as possible.)\n",
    "\n",
    "First, let's start by converting the sum in $\\mathcal{L}(\\beta)$ into a matrix multiplication. \n",
    "\n",
    "The most common loss function (and best, in most situations) is **squared error loss**, which looks like this:\n",
    "$$\\mathcal{L}(\\beta) = \\sum_{t=1}^T (y(t) - x(t) \\beta)^2 $$\n",
    "\n",
    "or, in matrix terms:\n",
    "\n",
    "$$ \\mathcal{L}(\\beta) = (Y - X\\beta)^\\top (Y - X \\beta) $$\n",
    "(If it's not clear why this is equivalent to our first expression for the loss function, try to re-imagine this equation as a summation across timepoints.)\n",
    "\n",
    "Using this function you can now try to find the best values for $\\beta$! Our goal here is to find the value of $\\beta$ that minimizes the loss function. Think back to when you first learned calculus.. do you remember learning how to find the minimum of a function? In particular, our loss function is actually a parabola (why?), which means it has (approximately) one minimum! We can find this minimum by taking the derivative of the loss function with respect to $\\beta$, and then finding the value of $\\beta$ that sets the derivative to zero.\n",
    "\n",
    "We can expand the multiplication in our loss function:\n",
    "$$ \\mathcal{L}(\\beta) = Y^\\top Y - 2 Y^\\top X \\beta + \\beta^\\top X^\\top X \\beta $$\n",
    "Taking the derivative then gives us this expression:\n",
    "$$\\frac{d \\mathcal{L}(\\beta)}{d \\beta} = -2 Y^\\top X + 2 \\beta^\\top X^\\top X $$\n",
    "Here, the $Y^\\top Y$ term disappeared because it didn't depend on $\\beta$. If you want to know how the other terms came about, you might want to peruse the [somewhat baroque rules of matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus).\n",
    "\n",
    "Now to find the best value of $\\beta$ we need to set $\\frac{d \\mathcal{L}(\\beta)}{d \\beta} = 0$ and then solve for $\\beta$:\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{d \\mathcal{L}(\\beta)}{d \\beta} &=& 0 \\\\\n",
    "-2 Y^\\top X + 2 \\beta^\\top X^\\top X &=& 0 \\\\\n",
    "\\beta^\\top X^\\top X &=& Y^\\top X \\\\\n",
    "X^\\top X \\beta &=& X^\\top Y \\mbox{  (transposed both sides)} \\\\\n",
    "\\beta &=& (X^\\top X)^{-1} X^\\top Y\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "And there we have it! The best possible weights for predicting $y(t)$ from $x(t)$ are given by this neat little expression: $\\beta = (X^\\top X)^{-1} X^\\top Y$.\n",
    "\n",
    "Going forward, we're going to refer to this regression procedure as **_ordinary least squares_** or **_OLS_**, and the solution that we derived as \"the OLS solution\" or $\\beta_{OLS}$.\n",
    "\n",
    "Let's try this procedure out on some fake data, then some real data from an fMRI experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's solve the regression exactly using the formula from above\n",
    "\n",
    "beta_estimate_1 = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(Y_train)\n",
    "print(\"estimated:\", beta_estimate_1)\n",
    "print(\"true:\", beta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outrageously important concept: _testing your model on held-out data_\n",
    "How do you know how good your regression model is? The estimated weights look _kind of_ like the true weights, but in the real world you don't know what the real weights are!\n",
    "\n",
    "One option would be to test how well your model fit your original (training) dataset. That would be a bad option. We'll get into why in the next module.\n",
    "\n",
    "The _correct_ option is to test how well your model can predict _new_ data, on which the model was not trained. Let's do that here, by generating some more data using our 'true' weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new data to test the model on\n",
    "\n",
    "T_test = 25 # number of timepoints for our new test dataset\n",
    "\n",
    "X_test = np.random.randn(T_test, p)\n",
    "Y_test = X_test.dot(beta_true) + noise_size * np.random.randn(T_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we evaluate our model?\n",
    "There are many ways! We're going to use one: [linear correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test how well this model is able to predict the held-out test data\n",
    "Y_test_pred = X_test.dot(beta_estimate_1)\n",
    "\n",
    "print(\"Correlation between true & predicted Y_test:\", np.corrcoef(Y_test, Y_test_pred)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice you should really not use the formula that has an explicit inverse for doing OLS\n",
    "# this has to do with numerical stability, something that ideally you shouldn't concern yourself with\n",
    "# instead we'll use a function that numpy includes explicitly for this purpose!\n",
    "# at its core, it uses a singular value decomposition (SVD) instead of explicit matrix inversion\n",
    "# this is much more stable!\n",
    "\n",
    "beta_estimate_2 = np.linalg.lstsq(X_train, Y_train)[0]\n",
    "print(\"estimated:\", beta_estimate_2)\n",
    "print(\"true:\", beta_true)\n",
    "\n",
    "# (for this simple case, though, the answer should be ~identical to what you found previously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS for an fMRI analysis\n",
    "\n",
    "Let's try to use this method now for an [fMRI experiment](https://www.nature.com/articles/nature17637). In this experiment, each subject listened to 11 different natural, narrative stories over headphones while we recorded BOLD signals continuously (well, every 2 seconds) using fMRI. The first 10 of these stories will comprise the _training set_, on which we will learn our regression models. The last story will comprise the _test set_, on which we will test our models.\n",
    "\n",
    "The model that we are going to fit is quite simple. Our stimulus (what we call $s(t)$) is a set of audio stories. The features that we have extracted from these stories (what we call $x(t)$) are indicator variables that show which words were spoken at each timepoint (here timepoints correspond to the fMRI volumes, so each one represents a 2 second period).\n",
    "\n",
    "We're going to skip a lot of the preprocessing steps here and load data that's _almost_ already set up for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's load up the list of unique words that appear in the stories\n",
    "import tables\n",
    "word_tf = tables.open_file('data/we_word_embeddings/small_english1000sm.hdf5')\n",
    "words = word_tf.root.vocab.read()\n",
    "word_tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's load up the feature matrices\n",
    "# these were stored as \"sparse\" matrices in order to save space\n",
    "# but we'll convert them back to normal matrices in order to use them in our regression\n",
    "from scipy import sparse\n",
    "training_features = sparse.load_npz('data/we_word_embeddings/indicator_Rstim.npz').todense().A\n",
    "test_features = sparse.load_npz('data/we_word_embeddings/indicator_Pstim.npz').todense().A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features.shape, test_features.shape\n",
    "# these should have different numbers of timepoints (3737 for training, 291 for test)\n",
    "# but the same number of feature dimensions for both (2702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see which words appeared in each timepoint of the fMRI data like this\n",
    "# (this is a list of all the words appearing in a 2-second segment, out of order!)\n",
    "words[training_features[123] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's load up the brain responses\n",
    "response_tf = tables.open_file('data/we_word_embeddings/small-fmri-responses.hdf5')\n",
    "training_resp = response_tf.root.zRresp.read()\n",
    "test_resp = response_tf.root.zPresp.read()\n",
    "brain_mask = response_tf.root.mask.read()\n",
    "response_tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resp.shape, test_resp.shape\n",
    "# these should have the same number of timepoints as the features (3737 for training, 291 for test)\n",
    "# the other dimension is the number of voxels (5156 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to accurately predict BOLD responses we need to account for hemodynamic delays\n",
    "# we'll do that here by creating multiple time-shifted versions of the same stimulus\n",
    "# this is called a finite impulse response or FIR model\n",
    "\n",
    "from util import make_delayed\n",
    "delays = [1,2,3,4]\n",
    "\n",
    "del_training_features = make_delayed(training_features, delays)\n",
    "del_test_features = make_delayed(test_features, delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_training_features.shape, del_test_features.shape\n",
    "# these should have the same number of timepoints as before, but now the number of features\n",
    "# should be increased by a factor of 4 (to 10808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can fit our regression models!\n",
    "\n",
    "beta_ols = np.linalg.lstsq(del_training_features, training_resp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ols.shape\n",
    "# should be total number of features (10808) by number of voxels (5156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test our regression models on the held-out test data\n",
    "pred_test_resp = del_test_features.dot(beta_ols)\n",
    "\n",
    "import npp # a set of convenience functions I think are missing from numpy :)\n",
    "\n",
    "test_correlations = npp.mcorr(test_resp, pred_test_resp) # computes the correlation for each voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correlations.shape\n",
    "# should be a vector with one value per voxel (5156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the histogram of correlations!\n",
    "plt.hist(test_correlations, 50)\n",
    "plt.xlim(-1, 1)\n",
    "plt.xlabel(\"Linear Correlation\")\n",
    "plt.ylabel(\"Num. Voxels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yikes that doesn't look too good! It doesn't seem like we were able to predict much of anything here :(\n",
    "# to be sure, let's look at a brain map of the correlations!\n",
    "\n",
    "import cortex\n",
    "\n",
    "corr_volume = cortex.Volume(test_correlations, 'S1', 'fullhead', mask=brain_mask, vmin=-0.3, vmax=0.3, cmap='RdBu_r')\n",
    "cortex.quickshow(corr_volume);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also look at it in 3D!\n",
    "\n",
    "cortex.webshow(corr_volume, open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for OLS 👎\n",
    "It did not work well. Let's improve things in the next module!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
